import os
import shutil
from urllib.request import urlopen
import urllib.request
from urllib import parse
import re
from bs4 import BeautifulSoup
import pandas as pd
import time
#
# if os.path.exists('../python/oaislib_org.py'):
#     shutil.copy('../python/oaislib_org.py', 'oaislib.py')

#ps20-10
#ìœ íŠœë¸Œ ìœ íŠœë¸Œ ì¸ê¸°ë™ì˜ìƒ í˜ì´ì§€ì—ì„œ ì œëª©ì„ ê°€ì ¸ì˜¤ëŠ” ì½”ë“œ
def ps20_10_youtube_trend_title():
    def fn_get_clean_text(text):
        text = str(text)
        text = text.replace("\\", "")  # ìœ„ì˜ í‘œí˜„ìœ¼ë¡œ ì—­ìŠ¬ë˜ì‹œê°€ ì œê±°ê°€ ì•ˆë˜ì–´ì„œ ì¶”ê°€í•¨
        text = re.sub('[-=+,#/\?:^â­â˜†ï¼‚ã…£â€™$.,ğŸ’«â€"@ğŸ•º*\"â€»~ğŸ’‰&%ğŸ˜ğŸ¥¶ğŸ¤¬ğŸ’šğŸšâ™¨â˜â™¬ã†!ã…¡ã€\\;_â€˜|\(\)\[\]\<\>`\'â€¦ã€‹`]', '', text)
        return text

    output_df = pd.DataFrame()
    # I/O
    target_url = "https://www.youtube.com/feed/trending"
    output_filepath = 'output/ps20-10/youtube_trend_title.csv'

    #Crawling
    html = urllib.request.urlopen(target_url).read()
    soup = BeautifulSoup(html, 'html.parser')
    #íƒ€ì´í‹€ ê°€ì ¸ì˜¤ê¸° ì´ ì—¬ê¸° ì •ê·œí‘œí˜„ì‹ì´ ê¹”ë”í•˜ê²Œ ì²˜ë¦¬ê°€ ì•ˆë¨..
    pattern = 'title":{.*?}'
    #pattern = 'title":"(.*?)"'
    result_title = re.findall(pattern, str(soup), re.MULTILINE | re.IGNORECASE)
    trend_title = [x[25:-2] for x in result_title]
    print(trend_title)
    print('')

    #íŠ¹ìˆ˜ë¬¸ì ì‚­ì œí•˜ëŠ” í•¨ìˆ˜
    trend_title = fn_get_clean_text(trend_title)
    print(trend_title)

    # ë„ì–´ì“°ê¸° ê¸°ì¤€(ì–´ì ˆ) ë‚˜ëˆ„ê¸°
    youtube_trend_title = trend_title.split()

    ##í•œ ê¸€ìë§Œ ìˆëŠ”ê²½ìš° ì‚­ì œ
    for i in range(len(youtube_trend_title)):
        #print(type(youtube_trend_title[i]))
        if len(youtube_trend_title[i]) <= 1:
            youtube_trend_title[i] = ''
    #ì§‘í•©í˜•ìœ¼ë¡œ ì¤‘ë³µë¬¸ì ì‚­ì œ í›„ ë‹¤ì‹œ ë¦¬ìŠ¤íŠ¸ ë³€í™˜
    my_set = set(youtube_trend_title)
    youtube_trend_title = list(my_set)
    #ê³µë°±ë¬¸ì ì‚­ì œ
    youtube_trend_title = [x for x in youtube_trend_title if x]

    #ë‚ ì§œ ë„£ê¸°
    search_date = pd.datetime.now() # ë˜ê¸´í•˜ëŠ”ë° ë‹¤ë¥¸ê±¸ë¡œ ë°”ê¿”ì•¼ë¨.
    youtube_search_date = search_date.date()
    print(youtube_search_date)
    print('')
    trend_title_df = pd.DataFrame(columns=['date', 'youtube_trend_title'])
    trend_title_df['youtube_trend_title']= youtube_trend_title
    trend_title_df['date'] = youtube_search_date

    output_df = pd.concat([output_df, trend_title_df])

    if os.path.isfile(output_filepath):
        print("yes exist")
        output_df.to_csv(output_filepath, mode='a', header=False, index=False)
    else:
        print('nothing, makemake')
        output_df.to_csv(output_filepath, index=False)
    return output_df

#ps20-20
#20-10ì—ì„œ ê°€ì ¸ì˜¨ youtube_trend_title(keyword)ë¡œ ê²€ìƒ‰í–ˆì„ ë•Œ í™”ë©´ì— ë³´ì´ëŠ” ë™ì˜ìƒë“¤ì˜ ìœ íŠœë¸Œ ì•„ì´ë”” ìˆ˜ì§‘
def ps20_20_get_yid_by_search_df():
    input_filepath = 'output/ps20-10/youtube_trend_title.csv'
    youtube_trend_title_df = pd.read_csv(input_filepath)
    output_filepath = 'output/ps20-20/youtube_ids.csv'
    output_df = pd.DataFrame()
    for i in range(len(youtube_trend_title_df)):
        print(i)
        keyword = youtube_trend_title_df['youtube_trend_title'].iloc[i]
        keyword = keyword.encode('utf-8')
        keyword = str(keyword)

        target_url='https://www.youtube.com/results?search_query=' + keyword
        html = urllib.request.urlopen(target_url).read()
        soup = BeautifulSoup(html, 'html.parser')
        pattern = '"\/watch\?v=.{11}"'
        result = re.findall(pattern, str(soup), re.MULTILINE | re.IGNORECASE)
        youtube_ids = [x[10:-1] for x in result]
        youtube_urls = ["https://www.youtube.com/watch?v=" + x for x in youtube_ids]
        youtube_df = pd.DataFrame(columns = ['youtube_id', 'youtube_url', 'keyword', 'date'])
        youtube_df['youtube_id'] = youtube_ids
        youtube_df['youtube_url'] = youtube_urls
        youtube_df['keyword'] = youtube_trend_title_df['youtube_trend_title'].iloc[i]
        youtube_df['date'] = youtube_trend_title_df['date']
        output_df = pd.concat([output_df, youtube_df])
        print('')


    output_df.drop_duplicates(inplace=True, subset=['youtube_id', 'youtube_url'])
    if os.path.isfile(output_filepath):
        print("yes exist")
        output_df.to_csv(output_filepath, mode='a', header=False, index=False)
    else:
        print('nothing, makemake')
        output_df.to_csv(output_filepath, index=False)
    print('')
    return output_df

#ps20-30
#20-20ì—ì„œ ê°€ì ¸ì˜¨ ìœ íŠœë¸Œ ì•„ì´ë”” ê°’ìœ¼ë¡œ ì›¹í˜ì´ì§€ ì ‘ì†í–ˆì„ë•Œ
#ì˜†ì— ë³´ì´ëŠ” ê´€ë ¨ë™ì˜ìƒ ë¶€ë¶„ì˜ ìœ íŠœë¸Œ ì•„ì´ë””ë¥¼ ê°€ì ¸ ì˜¤ëŠ” ì½”ë“œ
def ps20_30_get_additional_yid():
    # I/O
    input_filepath = 'output/ps20-20/youtube_ids.csv'
    youtube_ids_df = pd.read_csv(input_filepath)
    output_filepath = 'output/ps20-30/youtube_ids_additional.csv'
    output_df = pd.DataFrame()
    for i in range(len(youtube_ids_df)):
        print(i)
        url_str = youtube_ids_df['youtube_url'].iloc[i]
        keyword_str = youtube_ids_df['keyword'].iloc[i]
        target_url = url_str
        html = urllib.request.urlopen(target_url).read()
        soup = BeautifulSoup(html, 'html.parser')
        pattern = '"videoId":".{11}"'
        result = re.findall(pattern, str(soup), re.MULTILINE | re.IGNORECASE)
        youtube_ids = [x[11:-1] for x in result]
        youtube_urls = ["https://www.youtube.com/watch?v=" + x for x in youtube_ids]
        youtube_df = pd.DataFrame(columns=['youtube_id', 'youtube_url','keyword'])  # ps10-30 ë°ì´í„° í”„ë ˆì„ ìƒì„±
        youtube_df['youtube_id'] = youtube_ids
        youtube_df['youtube_url'] = youtube_urls
        youtube_df['keyword'] = keyword_str

        output_df = pd.concat([output_df, youtube_df])
        print('')
    #forë¬¸ ë²—ì–´ë‚˜ì„œ ì—´ì„ ì¶”ê°€í•´ì„œ ì½”ë“œ ëŒë¦° ë‚ ì§œë¥¼ ë„£ê¸°
    search_date = pd.datetime.now()  # ë˜ê¸´í•˜ëŠ”ë° ë‹¤ë¥¸ê±¸ë¡œ ë°”ê¿”ì•¼ë¨.
    youtube_search_date = search_date.date()
    print(youtube_search_date)
    print('')
    output_df['search date'] = youtube_search_date

    output_df.drop_duplicates(inplace=True, subset=['youtube_id', 'youtube_url'])
    if os.path.isfile(output_filepath):
        print("yes exist")
        output_df.to_csv(output_filepath, mode='a', header=False, index=False)
    else:
        print('nothing, makemake')
        output_df.to_csv(output_filepath, index=False)
    print('')
    return output_df

#ì—¬íƒœ ìˆ˜ì§‘í•œ ì—¬ëŸ¬ ìë£Œë“¤ì˜ ë©”íƒ€ì •ë³´ ìˆ˜ì§‘í•˜ê³  ì €ì¥í•˜ëŠ” ì½”ë“œ
def ps20_40_getmetadata():
    # I/O
    input_filepath = 'output/ps20-30/youtube_ids_additional.csv'
    youtube_metadata_df = pd.read_csv(input_filepath)
    output_filepath = 'output/ps20-40/youtube_get_metadata.csv'

    output_df = pd.DataFrame()
    for i in range(len(youtube_metadata_df)):
        print(i)
        url_str = youtube_metadata_df['youtube_url'].iloc[i]
        target_url = url_str
        print(target_url)
        html = urllib.request.urlopen(target_url).read()
        soup = BeautifulSoup(html, 'html.parser')

        #ìœ íŠœë¸Œ id í¬ë¡¤ë§
        #pattern = '"videoId":".{11}"'
        pattern = 'videoId":"(.*?)"'
        result_id = re.findall(pattern, str(soup), re.MULTILINE | re.IGNORECASE)
        #youtube_ids = [x[11:-1] for x in result_id]
        if len(result_id) >= 1:
            result_youtube_id = result_id[0]
            print(result_youtube_id)
        else:
            result_youtube_id = -1

        #íƒ€ì´í‹€ í¬ë¡¤ë§
        pattern_title = 'title":"(.*?)"'
        result_title = re.findall(pattern_title, str(soup), re.MULTILINE | re.IGNORECASE)
        if len(result_title) >= 1:
            result_youtube_title = result_title[0].split(",")[0]# íŒ¨í„´ì—ì„œ title: ë¶€ë¶„ ì§€ìš°ê³ , lengthSecond ì „ê¹Œì§€ ì˜ë¼ë‚´ê¸°
            print(result_youtube_title)  # ë‚˜ì¤‘ì— ì‚­ì œí•´ë„ ë˜ëŠ”ë¶€ë¶„. í™•ì¸ìš©ì´ì—ˆìŒ
        else:
            result_youtube_title = -1

        #ì¡°íšŒìˆ˜ í¬ë¡¤ë§
        pattern_viewcount = 'viewCount":"(.*?)"'
        result_viewcount = re.findall(pattern_viewcount, str(soup), re.MULTILINE | re.IGNORECASE)
        if len(result_viewcount) >= 1 :
            result_youtube_viewcount = result_viewcount[0]
            print(result_youtube_viewcount)
        else:
            result_youtube_viewcount = -1

        #ì—…ë¡œë“œë‚ ì§œ í¬ë¡¤ë§
        pattern_upload = 'uploadDate":"(.*?)"'
        result_upload = re.findall(pattern_upload, str(soup), re.MULTILINE | re.IGNORECASE)
        if len(result_upload) >= 1 :
            result_youtube_upload = result_upload[0]
            print(result_youtube_upload)
        else:
            result_youtube_upload = -1
        print('')

        #ì±„ë„ë“±ë¡ì í¬ë¡¤ë§
        pattern_owner = 'ownerChannelName":"(.*?)",'
        result_owner = re.findall(pattern_owner, str(soup))
        if len(result_owner) >= 1:
            result_youtube_owner = result_owner[0]
            print(result_youtube_owner)
        else:
            result_youtube_owner = -1

        #ìœ íŠœë¸Œ tag(keyword) í¬ë¡¤ë§
        pattern_tag =  'keywords\\":\[.*?\]'
        tag = re.findall(pattern_tag, str(soup), re.MULTILINE | re.IGNORECASE)
        result_tag = [y[11:-1] for y in tag]
        if len(result_tag) >= 1:
            result_youtube_tag = result_tag[0]
            print(result_youtube_tag)
        else:
            result_youtube_tag = -1
        print('')

        #youtube_metainfo_df = pd.DataFrame(columns=['youtube_id','youtube_url','title','watch_cnt','upload_date', 'channelOwner', 'Tag'])
        new_row = {'youtube_id':result_youtube_id,
                   'youtube_url':target_url,
                   'title':result_youtube_title,
                   'watch_cnt':result_youtube_viewcount,
                   'upload_date':result_youtube_upload,
                   'channelOwner': result_youtube_owner,
                   'Tag':result_youtube_tag}

        output_df = output_df.append(new_row, ignore_index=True)
        print('')
    # forë¬¸ ë²—ì–´ë‚˜ì„œ ì—´ì„ ì¶”ê°€í•´ì„œ ì½”ë“œ ëŒë¦° ë‚ ì§œë¥¼ ë„£ê¸°
    search_date = pd.datetime.now()  # ë˜ê¸´í•˜ëŠ”ë° ë‹¤ë¥¸ê±¸ë¡œ ë°”ê¿”ì•¼ë¨.
    youtube_search_date = search_date.date()
    print(youtube_search_date)
    print('')
    output_df['search date'] = youtube_search_date

    output_df.drop_duplicates(inplace=True, subset=['youtube_id', 'youtube_url'])
    if os.path.isfile(output_filepath):
        print("yes exist")
        output_df.to_csv(output_filepath, mode='a', header=False, index=False)
    else:
        print('nothing, makemake')
        output_df.to_csv(output_filepath, index=False)
    print('')
    return output_df

if __name__ == '__main__':
    ps20_10_youtube_trend_title()
    ps20_20_get_yid_by_search_df()
    ps20_30_get_additional_yid()
    ps20_40_getmetadata()